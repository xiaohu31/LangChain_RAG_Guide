# LangChain RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) å®ç°å®Œæ•´æŒ‡å—

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨ LangChain æ¡†æ¶ä»é›¶æ„å»ºä¸€ä¸ª RAG ç³»ç»Ÿã€‚æ¶µç›–äº†ä»æ–‡æ¡£åŠ è½½ã€åˆ‡åˆ†ã€å‘é‡åŒ–ã€å­˜å‚¨åˆ°æ£€ç´¢å’Œç”Ÿæˆçš„å…¨æµç¨‹ï¼Œå¹¶é™„å¸¦äº†è¯¦ç»†çš„ä»£ç æ³¨é‡Šã€‚

---

## ğŸ“š ç›®å½•

- [ä¸€ã€RAG æ ¸å¿ƒæµç¨‹](#ä¸€rag-æ ¸å¿ƒæµç¨‹)
- [äºŒã€è¯¦ç»†å®ç°æ­¥éª¤](#äºŒè¯¦ç»†å®ç°æ­¥éª¤)
  - [0. ç¯å¢ƒå‡†å¤‡ä¸ä¾èµ–å®‰è£…](#0-ç¯å¢ƒå‡†å¤‡ä¸ä¾èµ–å®‰è£…)
  - [1. æ–‡æ¡£åŠ è½½](#1-æ–‡æ¡£åŠ è½½-document-loading)
  - [2. æ–‡æœ¬åˆ†å‰²](#2-æ–‡æœ¬åˆ†å‰²-text-splitting)
  - [3. æ–‡æœ¬å‘é‡åŒ–](#3-æ–‡æœ¬å‘é‡åŒ–-embeddings)
  - [4. å‘é‡æ•°æ®åº“å­˜å‚¨](#4-å‘é‡æ•°æ®åº“å­˜å‚¨-vector-stores)
  - [5. æ£€ç´¢ä¸ LLM ç”Ÿæˆ](#5-æ£€ç´¢ä¸-llm-ç”Ÿæˆ-rag-chain)
- [ä¸‰ã€é«˜çº§åŠŸèƒ½ä¸è¿›é˜¶](#ä¸‰é«˜çº§åŠŸèƒ½ä¸è¿›é˜¶)
  - [1. å¸¦è®°å¿†çš„å¯¹è¯é“¾](#1-å¸¦è®°å¿†çš„å¯¹è¯é“¾-conversational-rag)
  - [2. é«˜çº§æ£€ç´¢ç­–ç•¥](#2-é«˜çº§æ£€ç´¢ç­–ç•¥-advanced-retrieval)
  - [3. é‡æ’åº (Reranking)](#3-é‡æ’åº-reranking---æå‡ç²¾åº¦çš„æœ€åä¸€æ­¥)
  - [4. çˆ¶æ–‡æ¡£æ£€ç´¢å™¨](#4-çˆ¶æ–‡æ¡£æ£€ç´¢å™¨-parent-document-retriever---è§£å†³é¢—ç²’åº¦çŸ›ç›¾)
  - [5. å¤šæŸ¥è¯¢æ£€ç´¢](#5-å¤šæŸ¥è¯¢æ£€ç´¢-multi-query-retrieval---å¤„ç†æé—®ä¸å½“)
  - [6. RAG æ•ˆæœè¯„ä¼°](#6-rag-æ•ˆæœè¯„ä¼°-evaluation---æ‹’ç»ç›²ç›®è°ƒä¼˜)
  - [7. ç´¢å¼• API](#7-ç´¢å¼•-api-indexing-api---ç”Ÿäº§ç¯å¢ƒå¿…å¤‡)
  - [8. ç»“æ„åŒ–è¾“å‡º](#8-ç»“æ„åŒ–è¾“å‡º-structured-output---ç°ä»£-rag-çš„æ ‡é…)
  - [9. ä»£ç†å‹ RAG ä¸ create_agent](#9-ä»£ç†å‹-rag-ä¸-create_agent--langchain-v10-æ–°æ ‡å‡†)
  - [10. æŸ¥è¯¢åˆ†æä¸è¯­ä¹‰è·¯ç”±](#10-æŸ¥è¯¢åˆ†æä¸è¯­ä¹‰è·¯ç”±-query-analysis--routing)
  - [11. å¤šæ¨¡æ€ RAG](#11-å¤šæ¨¡æ€-rag-multimodal-rag---2026-å¹´å‰æ²¿è¶‹åŠ¿)
  - [12. LangSmith å¯è§‚æµ‹æ€§](#12-langsmith-å¯è§‚æµ‹æ€§---ç”Ÿäº§ç¯å¢ƒå¿…å¤‡)
- [å››ã€2026 å¹´ RAG å‰æ²¿æŠ€æœ¯](#å››2026-å¹´-rag-å‰æ²¿æŠ€æœ¯-cutting-edge)
- [äº”ã€RAG æ€§èƒ½è°ƒä¼˜ Checklist](#äº”æ€»ç»“rag-æ€§èƒ½è°ƒä¼˜-checklist-2026-æ›´æ–°ç‰ˆ)
- [å…­ã€æƒå¨å‚è€ƒä¸å®˜æ–¹æ–‡æ¡£é“¾æ¥](#å…­æƒå¨å‚è€ƒä¸å®˜æ–¹æ–‡æ¡£é“¾æ¥)

---

## ğŸš€ å¿«é€Ÿå…¥é—¨ (30ç§’ä¸Šæ‰‹)

ä»¥ä¸‹æ˜¯ä¸€ä¸ªæœ€ç®€ RAG ç¤ºä¾‹ï¼Œå¸®åŠ©ä½ å¿«é€Ÿç†è§£æ ¸å¿ƒæµç¨‹ï¼š

```python
# å®‰è£…ä¾èµ–: pip install langchain langchain-openai langchain-community chromadb langchain_text_splitters

from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# 1. åŠ è½½æ–‡æ¡£
docs = TextLoader("./your_document.txt").load()

# 2. åˆ†å‰²æ–‡æ¡£
splits = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50).split_documents(docs)

# 3. åˆ›å»ºå‘é‡åº“
vectordb = Chroma.from_documents(splits, OpenAIEmbeddings())

# 4. æ„å»º RAG é“¾
prompt = ChatPromptTemplate.from_template("æ ¹æ®ä»¥ä¸‹å†…å®¹å›ç­”é—®é¢˜:\n{context}\n\né—®é¢˜: {question}")
rag_chain = (
    {"context": vectordb.as_retriever() | (lambda docs: "\n".join(d.page_content for d in docs)), 
     "question": RunnablePassthrough()}
    | prompt | ChatOpenAI(model="gpt-4o-mini") | StrOutputParser()
)

# 5. æé—®
print(rag_chain.invoke("æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ"))
```

---

## ä¸€ã€RAG æ ¸å¿ƒæµç¨‹

RAG (Retrieval-Augmented Generation) çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šå…ˆæ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œå†è¾…åŠ©å¤§æ¨¡å‹ç”Ÿæˆç­”æ¡ˆã€‚

**æµç¨‹å›¾è§£:**
1. **Load (åŠ è½½)**: å°† PDFã€Wordã€Markdown ç­‰æ–‡ä»¶åŠ è½½ä¸ºæ–‡æœ¬ã€‚
2. **Split (åˆ†å‰²)**: å°†é•¿æ–‡æœ¬åˆ†å‰²ä¸ºè¾ƒå°çš„å— (Chunks)ã€‚
3. **Embed (å‘é‡åŒ–)**: å°†æ–‡æœ¬å—è½¬æ¢ä¸ºæ•°å€¼å‘é‡ã€‚
4. **Store (å­˜å‚¨)**: å°†å‘é‡å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ (Vector DB)ã€‚
5. **Retrieve (æ£€ç´¢)**: æ ¹æ®ç”¨æˆ·é—®é¢˜ï¼Œåœ¨å‘é‡åº“ä¸­æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„æ–‡æœ¬å—ã€‚
6. **Generate (ç”Ÿæˆ)**: å°†æ£€ç´¢åˆ°çš„æ–‡æœ¬å—ä½œä¸ºâ€œä¸Šä¸‹æ–‡â€å–‚ç»™ LLMï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚

---

## äºŒã€è¯¦ç»†å®ç°æ­¥éª¤

### 0. ç¯å¢ƒå‡†å¤‡ä¸ä¾èµ–å®‰è£…

é¦–å…ˆå®‰è£… LangChain ç”Ÿæ€ç³»ç»Ÿçš„æ ¸å¿ƒåº“å’Œå¸¸ç”¨ç»„ä»¶ã€‚

```bash
# langchain: æ ¸å¿ƒæ¡†æ¶ (v1.0+)
# langchain-community: ç¤¾åŒºç»„ä»¶(åŒ…å«å„ç§ loaders, vectorstores)
# langchain-openai: OpenAI æ¨¡å‹å°è£…
# langchain-classic: ã€é‡è¦ã€‘ä¼ ç»Ÿé“¾å’Œæ£€ç´¢å™¨ï¼ˆå¦‚ LLMChain, MultiQueryRetriever ç­‰ï¼‰
# chromadb: å‘é‡æ•°æ®åº“
# pydantic: v1.0 å…¨é¢å‡çº§è‡³ Pydantic v2
pip install -U langchain langchain-community langchain-openai langchain-classic chromadb pydantic
```

> âš ï¸ **ç‰ˆæœ¬å…¼å®¹æ€§è­¦å‘Š (2026å¹´1æœˆæ›´æ–°)**:
> LangChain å·²äº **2025 å¹´ 10 æœˆ**æ­£å¼å‘å¸ƒ **v1.0**ã€‚è®¸å¤šä¼ ç»ŸåŠŸèƒ½ï¼ˆå¦‚ `ConversationalRetrievalChain`, `MultiQueryRetriever`, `RetrievalQA`, Indexing API ç­‰ï¼‰å·²ä»æ ¸å¿ƒ `langchain` åŒ…ç§»è‡³ **`langchain-classic`** åŒ…ã€‚
> - **å¦‚æœä½ ä½¿ç”¨ LangChain >= 1.0**: éœ€è¦å°† `from langchain.chains import ...` æ”¹ä¸º `from langchain_classic.chains import ...`ï¼Œå°† `from langchain.retrievers import MultiQueryRetriever` æ”¹ä¸º `from langchain_classic.retrievers import MultiQueryRetriever`ã€‚
> - **å¦‚æœä½ ä½¿ç”¨ LangChain 0.2.x / 0.3.x**: æœ¬ç¬”è®°ä¸­çš„æ—§å¯¼å…¥è·¯å¾„å¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚
> - **Text Splitter**: å·²è¿ç§»è‡³ç‹¬ç«‹åŒ… `langchain_text_splitters`ã€‚
> - è¯·å‚è€ƒå®˜æ–¹è¿ç§»æŒ‡å—: [LangChain v1 Migration Guide](https://python.langchain.com/docs/versions/v1/)

### 1. æ–‡æ¡£åŠ è½½ (Document Loading)

LangChain æä¾›äº†å¤šç§ `Loader` æ¥å¤„ç†ä¸åŒæ ¼å¼çš„æ–‡ä»¶ã€‚

```python
from langchain_community.document_loaders import (
    PyMuPDFLoader,              # ä¸“ç”¨äº PDF æ–‡ä»¶ï¼Œè§£æé€Ÿåº¦å¿«ï¼Œæ•ˆæœå¥½
    UnstructuredMarkdownLoader, # ç”¨äº Markdown æ–‡ä»¶
    TextLoader,                 # ç”¨äºçº¯æ–‡æœ¬æ–‡ä»¶ (.txt)
    WebBaseLoader               # ç”¨äºçˆ¬å–å’Œè§£æç½‘é¡µå†…å®¹
)

# 1.1 åŠ è½½ PDF æ–‡ä»¶
# PyMuPDFLoader ä¼šå°† PDF çš„æ¯ä¸€é¡µåŠ è½½ä¸ºä¸€ä¸ª Document å¯¹è±¡
# Document å¯¹è±¡åŒ…å« page_content (æ–‡æœ¬å†…å®¹) å’Œ metadata (å…ƒæ•°æ®ï¼Œå¦‚é¡µç ã€æ–‡ä»¶å)
pdf_loader = PyMuPDFLoader("./data/knowledge.pdf")
pdf_docs = pdf_loader.load()

# 1.2 åŠ è½½ Markdown æ–‡ä»¶
# UnstructuredMarkdownLoader ä¼šè§£æ Markdown ç»“æ„
md_loader = UnstructuredMarkdownLoader("./data/readme.md")
md_docs = md_loader.load()

# 1.3 åŠ è½½ç½‘é¡µ
# bs_kwargs ç”¨äºæŒ‡å®š BeautifulSoup çš„è§£æå‚æ•°ï¼Œè¿™é‡Œåªæå– article æ ‡ç­¾çš„å†…å®¹
from bs4 import SoupStrainer
web_loader = WebBaseLoader(
    web_paths=("https://example.com/article",),
    bs_kwargs=dict(parse_only=SoupStrainer("article")) 
)
web_docs = web_loader.load()

print(f"åŠ è½½äº† {len(pdf_docs)} é¡µ PDF æ–‡æ¡£")
```

### 2. æ–‡æœ¬åˆ†å‰² (Text Splitting)

å°†é•¿æ–‡æ¡£åˆ‡åˆ†ä¸ºè¾ƒå°çš„å— (Chunks)ï¼Œä»¥ä¾¿äº Embedding å’Œé€‚åº” LLM çš„ä¸Šä¸‹æ–‡çª—å£ã€‚

```python
# æ³¨æ„ï¼šLangChain v0.2+ èµ·ï¼Œtext_splitter å·²è¿ç§»è‡³ç‹¬ç«‹åŒ…
from langchain_text_splitters import RecursiveCharacterTextSplitter

# åˆå§‹åŒ–åˆ†å‰²å™¨
# RecursiveCharacterTextSplitter æ˜¯æœ€å¸¸ç”¨çš„åˆ†å‰²å™¨ï¼Œå®ƒä¼šé€’å½’åœ°å°è¯•æŒ‰åˆ†éš”ç¬¦åˆ—è¡¨è¿›è¡Œåˆ†å‰²ï¼Œ
# ä¼˜å…ˆä¿æŒæ®µè½ã€å¥å­çš„å®Œæ•´æ€§ã€‚
text_splitter = RecursiveCharacterTextSplitter(
    # chunk_size: æ¯ä¸ªåˆ†å—çš„æœ€å¤§å­—ç¬¦æ•°ã€‚å»ºè®®å€¼: 500-1000
    # å¤ªå°ä¼šå¯¼è‡´è¯­ä¹‰ç ´ç¢ï¼Œå¤ªå¤§ä¼šå¯¼è‡´æ£€ç´¢ä¸ç²¾å‡†
    chunk_size=500,
    
    # chunk_overlap: åˆ†å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°ã€‚
    # ä½œç”¨: ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§ï¼Œé¿å…å¥å­è¢«åˆ‡æ–­å¯¼è‡´è¯­ä¹‰ä¸¢å¤±ã€‚å»ºè®®å€¼: chunk_size çš„ 10%-20%
    chunk_overlap=50,
    
    # length_function: ç”¨äºè®¡ç®—é•¿åº¦çš„å‡½æ•°ï¼Œé»˜è®¤æ˜¯ len() è®¡ç®—å­—ç¬¦æ•°
    length_function=len,
    
    # separators: åˆ†éš”ç¬¦åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§ä»å·¦åˆ°å³å°è¯•åˆ†å‰²
    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ";", "ï¼›", " ", ""]
)

# æ‰§è¡Œåˆ†å‰²
# docs æ˜¯ä¸Šä¸€æ­¥åŠ è½½çš„æ–‡æ¡£åˆ—è¡¨
# split_documents æ–¹æ³•ä¼šè¿”å›ä¸€ä¸ªæ–°çš„ Document åˆ—è¡¨ï¼ŒåŒ…å«åˆ†å‰²åçš„æ–‡æœ¬å—
split_docs = text_splitter.split_documents(pdf_docs)

print(f"åˆ†å‰²åå…±æœ‰ {len(split_docs)} ä¸ªæ–‡æœ¬å—")
```

### 3. æ–‡æœ¬å‘é‡åŒ– (Embeddings)

é€‰æ‹©ä¸€ä¸ª Embedding æ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ã€‚

```python
# é€‰é¡¹ A: ä½¿ç”¨ OpenAI Embeddings (éœ€è¦ API Keyï¼Œæ•ˆæœå¥½ï¼Œæ”¶è´¹)
from langchain_openai import OpenAIEmbeddings

embedding_model = OpenAIEmbeddings(
    model="text-embedding-3-small", # OpenAI çš„æ–°ä¸€ä»£é«˜æ•ˆæ¨¡å‹
    openai_api_key="sk-..."         # ä½ çš„ OpenAI API Key
)

# é€‰é¡¹ B: ä½¿ç”¨æœ¬åœ° HuggingFace æ¨¡å‹ (å…è´¹ï¼Œéšç§å¥½ï¼Œéœ€è¦è®¡ç®—èµ„æº)
# 2026 å¹´æ¨èæ¨¡å‹:
#   - 'BAAI/bge-m3' (å¤šè¯­è¨€ + å¤šç²’åº¦ï¼Œ2025å¹´æ–°æ¨¡å‹)
#   - 'jinaai/jina-embeddings-v3' (å…¨çƒæ’åå‰åˆ—)
#   - 'moka-ai/m3e-base' (ä¸­æ–‡ç»å…¸æ¨¡å‹)
from langchain_huggingface import HuggingFaceEmbeddings

# model_kwargs={'device': 'cpu'} æŒ‡å®šè¿è¡Œè®¾å¤‡ï¼Œæœ‰ GPU å¯æ”¹ä¸º 'cuda'
embedding_model = HuggingFaceEmbeddings(
    model_name="moka-ai/m3e-base",
    model_kwargs={'device': 'cpu'} 
)
```

### 4. å‘é‡æ•°æ®åº“å­˜å‚¨ (Vector Stores)

å°†åˆ‡åˆ†å¥½çš„æ–‡æœ¬å—å’Œå¯¹åº”çš„å‘é‡å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ä¸­ã€‚

```python
from langchain_community.vectorstores import Chroma

# å®šä¹‰æŒä¹…åŒ–å­˜å‚¨è·¯å¾„ï¼Œè¿™æ ·é‡å¯ç¨‹åºåæ•°æ®ä¸ä¼šä¸¢å¤±
persist_directory = "./vector_db_data"

# åˆ›å»ºå¹¶ä¿å­˜å‘é‡åº“
# from_documents æ–¹æ³•ä¼šæ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
# 1. è°ƒç”¨ embedding_model å°† split_docs ä¸­çš„æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
# 2. å°†å‘é‡å’ŒåŸå§‹æ–‡æœ¬å­˜å‚¨åˆ° Chroma æ•°æ®åº“ä¸­
# 3. å°†æ•°æ®æŒä¹…åŒ–åˆ° persist_directory
vectordb = Chroma.from_documents(
    documents=split_docs,           # åˆ†å‰²åçš„æ–‡æ¡£åˆ—è¡¨
    embedding=embedding_model,      # ä½¿ç”¨çš„ Embedding æ¨¡å‹
    persist_directory=persist_directory # æŒä¹…åŒ–ç›®å½•
)

# å¦‚æœéœ€è¦åŠ è½½å·²å­˜åœ¨çš„å‘é‡åº“ï¼Œä½¿ç”¨ä»¥ä¸‹ä»£ç :
# vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding_model)

print("å‘é‡åº“åˆ›å»ºå®Œæˆå¹¶å·²æŒä¹…åŒ–")
```

### 5. æ£€ç´¢ä¸ LLM ç”Ÿæˆ (RAG Chain)

è¿™æ˜¯ RAG çš„æ ¸å¿ƒéƒ¨åˆ†ï¼šæ£€ç´¢ -> å¢å¼º -> ç”Ÿæˆã€‚

#### 5.1 é…ç½® LLM

```python
from langchain_openai import ChatOpenAI

# åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹
# model_name: æŒ‡å®šä½¿ç”¨çš„æ¨¡å‹ç‰ˆæœ¬
# temperature: æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ã€‚0 è¡¨ç¤ºæœ€ç¡®å®šã€æœ€äº‹å®ï¼›1 è¡¨ç¤ºæœ€æœ‰åˆ›æ„ã€‚
# RAG ä»»åŠ¡é€šå¸¸å»ºè®®è®¾ä¸º 0ï¼Œä»¥é˜²æ­¢æ¨¡å‹äº§ç”Ÿå¹»è§‰ã€‚
llm = ChatOpenAI(
    model_name="gpt-4o-mini",  # 2026å¹´æ€§ä»·æ¯”æœ€é«˜çš„æ¨¡å‹ (æˆ–ä½¿ç”¨ gpt-4o è·å¾—æœ€ä½³æ•ˆæœ)
    temperature=0,              
    openai_api_key="sk-..."
)
```

#### 5.2 æ„å»º Prompt æ¨¡æ¿

```python
from langchain_core.prompts import ChatPromptTemplate

# å®šä¹‰ Prompt æ¨¡æ¿
# {context}: å ä½ç¬¦ï¼Œå°†è¢«æ›¿æ¢ä¸ºæ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µ
# {question}: å ä½ç¬¦ï¼Œå°†è¢«æ›¿æ¢ä¸ºç”¨æˆ·çš„é—®é¢˜
template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åº“åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

è§„åˆ™:
1. å¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·ç›´æ¥è¯´"æˆ‘æ ¹æ®å·²çŸ¥ä¿¡æ¯æ— æ³•å›ç­”è¯¥é—®é¢˜"ï¼Œä¸è¦ç¼–é€ ã€‚
2. å›ç­”è¦ç®€æ´æ˜äº†ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯:
{context}

ç”¨æˆ·é—®é¢˜: {question}

å›ç­”:"""

prompt = ChatPromptTemplate.from_template(template)
```

#### 5.3 æ„å»º LCEL é“¾ (LangChain Expression Language)

LCEL æ˜¯ LangChain æ¨èçš„æ„å»ºæ–¹å¼ï¼Œå®ƒä½¿ç”¨ Linux ç®¡é“é£æ ¼çš„è¯­æ³• (`|`) å°†ç»„ä»¶è¿æ¥èµ·æ¥ã€‚

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# å°†å‘é‡åº“è½¬æ¢ä¸ºæ£€ç´¢å™¨ (Retriever)
# search_type="similarity": ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æœç´¢
# k=4: æ¯æ¬¡æ£€ç´¢è¿”å›æœ€ç›¸ä¼¼çš„ 4 ä¸ªæ–‡æ¡£å—
retriever = vectordb.as_retriever(search_type="similarity", search_kwargs={"k": 4})

def format_docs(docs):
    """è¾…åŠ©å‡½æ•°: å°†æ£€ç´¢åˆ°çš„ Document å¯¹è±¡åˆ—è¡¨è½¬æ¢ä¸ºçº¯å­—ç¬¦ä¸²ï¼Œç”¨æ¢è¡Œç¬¦è¿æ¥"""
    return "\n\n".join(doc.page_content for doc in docs)

# æ„å»º RAG æµæ°´çº¿
# å­—å…¸ä¸­çš„ key (context, question) å¯¹åº” Prompt æ¨¡æ¿ä¸­çš„å˜é‡å
rag_chain = (
    {
        "context": retriever | format_docs,  # æ­¥éª¤ 1: è°ƒç”¨æ£€ç´¢å™¨è·å–æ–‡æ¡£ï¼Œå¹¶æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
        "question": RunnablePassthrough()    # æ­¥éª¤ 2: ä¼ é€’ç”¨æˆ·åŸå§‹é—®é¢˜
    }
    | prompt                                 # æ­¥éª¤ 3: å°† context å’Œ question å¡«å……åˆ° Prompt æ¨¡æ¿
    | llm                                    # æ­¥éª¤ 4: å°†å®Œæ•´çš„ Prompt å‘é€ç»™ LLM
    | StrOutputParser()                      # æ­¥éª¤ 5: å°† LLM çš„è¾“å‡ºå¯¹è±¡è§£æä¸ºçº¯æ–‡æœ¬å­—ç¬¦ä¸²
)

# æ‰§è¡ŒæŸ¥è¯¢
query = "ä»€ä¹ˆæ˜¯ RAG æŠ€æœ¯ï¼Ÿ"
response = rag_chain.invoke(query)
print(f"é—®é¢˜: {query}")
print(f"å›ç­”: {response}")
```

---

## ä¸‰ã€é«˜çº§åŠŸèƒ½ä¸è¿›é˜¶

### 1. å¸¦è®°å¿†çš„å¯¹è¯é“¾ (Conversational RAG)

å¦‚æœéœ€è¦å¤šè½®å¯¹è¯ï¼ˆåŠ©æ‰‹è®°ä½ä¹‹å‰çš„èŠå¤©å†…å®¹ï¼‰ï¼Œéœ€è¦å¼•å…¥å†å²è®°å½•å¤„ç†ã€‚

```python
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# --- ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºå†å²æ„ŸçŸ¥æ£€ç´¢å™¨ ---
# ä½œç”¨ï¼šå¤„ç†ç”¨æˆ·é—®é¢˜ä¸­çš„ä»£è¯ï¼ˆå¦‚â€œå®ƒâ€ã€â€œè¿™ä¸ªâ€ï¼‰ï¼Œå°†å…¶ç»“åˆå†å²è®°å½•é‡å†™ä¸ºä¸€ä¸ªç‹¬ç«‹å®Œæ•´çš„æŸ¥è¯¢ã€‚

contextualize_q_system_prompt = """ç»™å®šèŠå¤©å†å²è®°å½•å’Œæœ€æ–°çš„ç”¨æˆ·é—®é¢˜ï¼ˆå¯èƒ½å¼•ç”¨äº†èŠå¤©å†å²ä¸­çš„ä¸Šä¸‹æ–‡ï¼‰ï¼Œ
è¯·æ„é€ ä¸€ä¸ªç‹¬ç«‹çš„é—®é¢˜ï¼Œä½¿å…¶åœ¨æ²¡æœ‰èŠå¤©å†å²çš„æƒ…å†µä¸‹ä¹Ÿèƒ½è¢«ç†è§£ã€‚
ä¸è¦å›ç­”é—®é¢˜ï¼Œåªéœ€é‡å†™å®ƒï¼Œå¦‚æœä¸éœ€è¦é‡å†™åˆ™åŸæ ·è¿”å›ã€‚"""

contextualize_q_prompt = ChatPromptTemplate.from_messages([
    ("system", contextualize_q_system_prompt),
    MessagesPlaceholder("chat_history"), # èŠå¤©å†å²å ä½ç¬¦
    ("human", "{input}"),                # ç”¨æˆ·æœ€æ–°é—®é¢˜
])

# create_history_aware_retriever ä¼šä½¿ç”¨ LLM æ¥é‡å†™æŸ¥è¯¢ï¼Œç„¶åä½¿ç”¨æ£€ç´¢å™¨è¿›è¡Œæ£€ç´¢
history_aware_retriever = create_history_aware_retriever(
    llm, retriever, contextualize_q_prompt
)

# --- ç¬¬äºŒæ­¥ï¼šåˆ›å»ºé—®ç­”é“¾ ---
# ä½œç”¨ï¼šæ ¹æ®æ£€ç´¢åˆ°çš„æ–‡æ¡£å›ç­”é—®é¢˜

qa_system_prompt = """ä½ æ˜¯ä¸€ä¸ªé—®ç­”åŠ©æ‰‹ã€‚ä½¿ç”¨ä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ã€‚
å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä¸çŸ¥é“ã€‚ä¿æŒå›ç­”ç®€æ´ã€‚

{context}"""

qa_prompt = ChatPromptTemplate.from_messages([
    ("system", qa_system_prompt),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
])

# create_stuff_documents_chain æ˜¯æœ€åŸºæœ¬çš„æ–‡æ¡£å¤„ç†é“¾ï¼Œå®ƒå°†æ‰€æœ‰æ–‡æ¡£æ‹¼æ¥åœ¨ä¸€èµ·æ”¾å…¥ Prompt
question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

# --- ç¬¬ä¸‰æ­¥ï¼šåˆ›å»ºæœ€ç»ˆçš„ RAG é“¾ ---
rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

# --- ä½¿ç”¨ç¤ºä¾‹ ---
from langchain_core.messages import HumanMessage, AIMessage

chat_history = [] # åˆå§‹åŒ–èŠå¤©è®°å½•

# ç¬¬ä¸€è½®
response1 = rag_chain.invoke({"input": "LangChain æ˜¯ä»€ä¹ˆ?", "chat_history": chat_history})
print(response1["answer"])

# æ›´æ–°å†å²
chat_history.extend([HumanMessage(content="LangChain æ˜¯ä»€ä¹ˆ?"), AIMessage(content=response1["answer"])])

# ç¬¬äºŒè½® (æŒ‡ä»£ "å®ƒ")
response2 = rag_chain.invoke({"input": "å®ƒæ”¯æŒ Python å—?", "chat_history": chat_history})
print(response2["answer"])
```

### 2. é«˜çº§æ£€ç´¢ç­–ç•¥ (Advanced Retrieval)

ä¸ºäº†æé«˜æ£€ç´¢å‡†ç¡®ç‡ï¼Œå¯ä»¥ä½¿ç”¨ **æ··åˆæ£€ç´¢ (Hybrid Search)**ã€‚

```python
# EnsembleRetriever ä»åœ¨ langchain.retrievers ä¸­
# BM25Retriever éœ€è¦ä» langchain_community å¯¼å…¥
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever

# 1. BM25 æ£€ç´¢å™¨: åŸºäºå…³é”®è¯åŒ¹é… (TF-IDF çš„æ”¹è¿›ç‰ˆ)
# ä¼˜åŠ¿: å¯¹ç²¾ç¡®åŒ¹é…ã€ä¸“æœ‰åè¯ã€ç‰¹å®šé”™è¯¯ä»£ç ç­‰æ•ˆæœæä½³
# åŠ£åŠ¿: æ— æ³•ç†è§£è¯­ä¹‰ (å¦‚ "å¼€å¿ƒ" å’Œ "é«˜å…´")
bm25_retriever = BM25Retriever.from_documents(split_docs)

# 2. å‘é‡æ£€ç´¢å™¨: åŸºäºè¯­ä¹‰åŒ¹é…
# ä¼˜åŠ¿: ç†è§£è¯­ä¹‰å…³ç³»
# åŠ£åŠ¿: å¯¹ç²¾ç¡®å…³é”®è¯å¯èƒ½ä¸å¦‚ BM25
vector_retriever = vectordb.as_retriever(search_kwargs={"k": 4})

# 3. æ··åˆæ£€ç´¢å™¨ (Ensemble)
# ä½œç”¨: ç»“åˆä¸¤è€…çš„ä¼˜ç‚¹ï¼Œé€šè¿‡åŠ æƒå¹³å‡å¾—å‡ºæœ€ç»ˆç»“æœ
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_retriever],
    weights=[0.5, 0.5] # æƒé‡è®¾ç½®ï¼Œå¯æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
)

# åœ¨ RAG é“¾ä¸­ä½¿ç”¨æ··åˆæ£€ç´¢å™¨æ›¿ä»£æ™®é€š retriever
rag_chain = (
    {"context": ensemble_retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)
```

### 3. é‡æ’åº (Reranking) - æå‡ç²¾åº¦çš„â€œæœ€åä¸€æ­¥â€

**çŸ¥è¯†ç‚¹è¯´æ˜**: å‘é‡æ£€ç´¢ï¼ˆç›¸ä¼¼åº¦æœç´¢ï¼‰åªæ ¹æ®è¯­ä¹‰è·ç¦»æ‰¾ Top-kï¼Œä½†å®ƒå¹¶ä¸çœŸæ­£â€œç†è§£â€é—®é¢˜ã€‚é‡æ’åºæ˜¯ä½¿ç”¨ä¸€ä¸ªæ›´å¼ºå¤§çš„æ¨¡å‹ï¼ˆCross-Encoderï¼‰å¯¹åˆç­›å‡ºçš„æ–‡æ¡£è¿›è¡Œæ‰“åˆ†ï¼Œç¡®ä¿æœ€ç›¸å…³çš„æ–‡æ¡£æ’åœ¨ç¬¬ä¸€ä½ã€‚

```python
# ContextualCompressionRetriever åœ¨ langchain.retrievers ä¸­
# FlashrankRerank æ˜¯ç¬¬ä¸‰æ–¹é›†æˆï¼Œåœ¨ langchain_community ä¸­
from langchain.retrievers import ContextualCompressionRetriever
from langchain_community.document_compressors import FlashrankRerank

# 1. åŸºç¡€æ£€ç´¢å™¨
base_retriever = vectordb.as_retriever(search_kwargs={"k": 10})

# 2. é…ç½®é‡æ’åºå™¨ (ä»¥ Flashrank ä¸ºä¾‹ï¼Œè½»é‡ä¸”å¿«é€Ÿ)
# ä½œç”¨: å°†åˆæ­¥ç­›é€‰çš„ 10 ä¸ªæ–‡æ¡£é‡æ–°æ’åˆ—ï¼Œåªç•™ä¸‹æœ€æœ‰ç”¨çš„ 3 ä¸ª
compressor = FlashrankRerank(model="ms-marco-Minilm-L-6-v2", top_n=3)

# 3. åˆ›å»ºå‹ç¼©æ£€ç´¢å™¨
rerank_retriever = ContextualCompressionRetriever(
    base_compressor=compressor, 
    base_retriever=base_retriever
)

# ä½¿ç”¨æ–¹å¼åŒæ™®é€š retriever
```

### 4. çˆ¶æ–‡æ¡£æ£€ç´¢å™¨ (Parent Document Retriever) - è§£å†³â€œé¢—ç²’åº¦â€çŸ›ç›¾

**çŸ¥è¯†ç‚¹è¯´æ˜**: è¿™æ˜¯ä¸€ä¸ªéå¸¸å®ç”¨çš„æŠ€å·§ã€‚
- **çŸ›ç›¾ç‚¹**: å°å— (Small Chunks) æ›´æœ‰åˆ©äºç²¾å‡†åŒ¹é…å‘é‡ï¼Œä½†å¤§æ¨¡å‹å›ç­”é—®é¢˜éœ€è¦å®Œæ•´çš„ä¸Šä¸‹æ–‡èƒŒæ™¯ã€‚
- **è§£å†³æ–¹æ¡ˆ**: å°†é•¿æ–‡æ¡£åˆ‡åˆ†ä¸ºâ€œçˆ¶å—â€å’Œâ€œå„çº§å­å—â€ã€‚å‘é‡åº“é‡Œå­˜å­å—ï¼Œæ£€ç´¢æ—¶åŒ¹é…åˆ°å­å—ï¼Œä½†è¿”å›ç»™å¤§æ¨¡å‹çš„æ˜¯å®ƒæ‰€å±çš„â€œçˆ¶å—â€å†…å®¹ã€‚

```python
# LangChain v1.0+: ParentDocumentRetriever å¯èƒ½éœ€è¦ä» langchain_classic å¯¼å…¥
# å¦‚ä½¿ç”¨ v1.0+: from langchain_classic.retrievers import ParentDocumentRetriever
from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import InMemoryStore

# å®šä¹‰çˆ¶å—å’Œå­å—çš„åˆ†å‰²å™¨ (æ³¨æ„ï¼šéœ€è¦ from langchain_text_splitters import RecursiveCharacterTextSplitter)
parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000) # çˆ¶å—å¤§
child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)   # å­å—å°

vectorstore = Chroma(collection_name="split_parents", embedding_function=embedding_model)
store = InMemoryStore() # å­˜å‚¨å®Œæ•´çš„çˆ¶æ–‡æ¡£å†…å®¹

retriever = ParentDocumentRetriever(
    vectorstore=vectorstore,
    docstore=store,
    child_splitter=child_splitter,
    parent_splitter=parent_splitter,
)

# æ·»åŠ æ–‡æ¡£æ—¶ï¼Œå®ƒä¼šè‡ªåŠ¨å®Œæˆä¸¤çº§åˆ†å‰²å’Œå…³è”
retriever.add_documents(pdf_docs, ids=None)
```

### 5. å¤šæŸ¥è¯¢æ£€ç´¢ (Multi-Query Retrieval) - å¤„ç†â€œæé—®ä¸å½“â€

**çŸ¥è¯†ç‚¹è¯´æ˜**: ç”¨æˆ·çš„é—®é¢˜å¾€å¾€æ¯”è¾ƒç®€çŸ­æˆ–æ¨¡ç³Šã€‚å¤šæŸ¥è¯¢æ³•åˆ©ç”¨ LLM å°†ç”¨æˆ·çš„ä¸€ä¸ªé—®é¢˜æ”¹å†™æˆ 3-5 ä¸ªä¸åŒè§’åº¦çš„æé—®ï¼Œåˆ†åˆ«å»åº“é‡Œæœï¼Œæœ€åæŠŠç»“æœå»é‡æ±‡æ€»ã€‚

```python
from langchain.retrievers.multi_query import MultiQueryRetriever
# LangChain v1.0+: éœ€ä» langchain_classic.retrievers.multi_query å¯¼å…¥

# åªéœ€è¦æŒ‡å®š LLM å’Œ åŸºç¡€æ£€ç´¢å™¨
multi_query_retriever = MultiQueryRetriever.from_llm(
    retriever=vectordb.as_retriever(), 
    llm=llm
)

# å®ƒä¼šè‡ªåŠ¨ç”Ÿæˆç±»ä¼¼â€œè¯·ä»æŠ€æœ¯è§’åº¦æè¿°...â€ â€œç®€è¿°...â€ç­‰å¤šä¸ªå˜ä½“
# æ³¨æ„ï¼šget_relevant_documents() å·²åºŸå¼ƒï¼Œv1.0 èµ·è¯·ä½¿ç”¨ invoke()
unique_docs = multi_query_retriever.invoke("RAGåŸç†")
```

### 6. RAG æ•ˆæœè¯„ä¼° (Evaluation) - æ‹’ç»ç›²ç›®è°ƒä¼˜

**çŸ¥è¯†ç‚¹è¯´æ˜**: æ­å»ºå®Œ RAG åï¼Œå¦‚ä½•é‡åŒ–å®ƒçš„è¡¨ç°ï¼Ÿä¸šç•Œé€šç”¨çš„è¯„ä¼°æ¡†æ¶æ˜¯ **Ragas**ã€‚å®ƒå…³æ³¨å››ä¸ªç»´åº¦ï¼ˆRagas Metricsï¼‰ï¼š
1. **å¿ å®åº¦ (Faithfulness)**: ç­”æ¡ˆæ˜¯å¦å®Œå…¨æ¥è‡ªäºæ£€ç´¢åˆ°çš„å†…å®¹ï¼Ÿï¼ˆé˜²æ­¢å¹»è§‰ï¼‰
2. **ç›¸å…³æ€§ (Answer Relevance)**: ç­”æ¡ˆæ˜¯å¦çœŸçš„å›ç­”äº†ç”¨æˆ·çš„é—®é¢˜ï¼Ÿ
3. **ä¸Šä¸‹æ–‡ç²¾åº¦ (Context Precision)**: æ£€ç´¢åˆ°çš„æ–‡æ¡£é‡Œï¼ŒçœŸæ­£æœ‰ç”¨çš„ä¿¡æ¯æ˜¯å¦æ’åœ¨å‰é¢ï¼Ÿ
4. **ä¸Šä¸‹æ–‡å¬å›ç‡ (Context Recall)**: æ£€ç´¢åˆ°çš„å†…å®¹æ˜¯å¦åŒ…å«äº†å›ç­”é—®é¢˜çš„å…¨éƒ¨å…³é”®ä¿¡æ¯ï¼Ÿ

> **å»ºè®®å·¥å…·**: `ragas` åº“ã€‚é€šè¿‡ LLM-as-a-Judgeï¼ˆè®©æ›´å¼ºçš„æ¨¡å‹å¦‚ GPT-4 æ¥ç»™å½“å‰æ¨¡å‹çš„å›ç­”æ‰“åˆ†ï¼‰æ¥å®ç°è‡ªåŠ¨åŒ–è¯„ä¼°ã€‚

```python
# Ragas è¯„ä¼°ç¤ºä¾‹
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall
from datasets import Dataset

# å‡†å¤‡è¯„ä¼°æ•°æ®é›†
eval_data = {
    "question": ["ä»€ä¹ˆæ˜¯ RAGï¼Ÿ"],
    "answer": ["RAG æ˜¯æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯..."],
    "contexts": [["RAG (Retrieval-Augmented Generation) çš„æ ¸å¿ƒæ€æƒ³æ˜¯..."]],
    "ground_truth": ["RAG æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„æŠ€æœ¯"]
}
eval_dataset = Dataset.from_dict(eval_data)

# æ‰§è¡Œè¯„ä¼°
result = evaluate(
    dataset=eval_dataset,
    metrics=[faithfulness, answer_relevancy, context_precision, context_recall]
)
print(result)  # è¾“å‡ºå„ç»´åº¦è¯„åˆ†
```

### 7. ç´¢å¼• API (Indexing API) - ç”Ÿäº§ç¯å¢ƒå¿…å¤‡
**æ ¸å¿ƒä»·å€¼**: å½“ä½ çš„æœ¬åœ°æ–‡ä»¶å‘ç”Ÿå˜åŠ¨ï¼ˆæ–°å¢ã€ä¿®æ”¹ã€åˆ é™¤ï¼‰æ—¶ï¼Œå¦‚æœä½ é‡æ–°è·‘ä¸€æ¬¡ `from_documents`ï¼Œä¼šé€ æˆå¤§é‡é‡å¤çš„å‘é‡å­˜å‚¨å’Œæ˜‚è´µçš„ API è´¹ç”¨ã€‚Indexing API ä¼šå¯¹æ¯”æ–‡ä»¶æŒ‡çº¹ï¼Œ**ä»…åŒæ­¥å˜åŠ¨éƒ¨åˆ†**ã€‚

```python
# LangChain v1.0+: Indexing API å·²ç§»è‡³ langchain-classic
# å¦‚ä½¿ç”¨ v1.0+: from langchain_classic.indexes import index
from langchain.indexes import index

# 1. å®šä¹‰ Record Manager (è®°å½•ç®¡ç†å™¨)ï¼Œé€šå¸¸å­˜åœ¨æœ¬åœ°æ•°æ®åº“
from langchain_community.indexes import SQLRecordManager
record_manager = SQLRecordManager("sqlite:///record_manager_cache.sql", namespace="my_rag_app")
record_manager.create_schema()

# 2. æ‰§è¡Œç´¢å¼•åŠ¨ä½œ (cleanup="incremental" ä»£è¡¨å¢é‡åŒæ­¥)
indexing_stats = index(
    split_docs,
    record_manager,
    vectordb,
    cleanup="incremental",
    source_id_key="source"
)
# è¿”å›å€¼åŒ…å«ï¼šnum_added, num_updated, num_deleted, num_skipped
```

### 8. ç»“æ„åŒ–è¾“å‡º (Structured Output) - ç°ä»£ RAG çš„æ ‡é…
**æ ¸å¿ƒä»·å€¼**: åœ¨ v0.3 ä¸­ï¼Œå®˜æ–¹æ¨èç›´æ¥å°† LLM è¾“å‡ºç»‘å®šåˆ° Pydantic æ¨¡å‹ï¼Œç¡®ä¿ä¸‹æ¸¸ç³»ç»Ÿå¯ä»¥ç¨³å®šè§£æç»“æœã€‚

```python
from pydantic import BaseModel, Field

# å®šä¹‰æœŸæœ›å¾—åˆ°çš„å›ç­”ç»“æ„
class AnswerSchema(BaseModel):
    answer: str = Field(description="å¯¹é—®é¢˜çš„æœ€ç»ˆå›ç­”")
    sources: list[str] = Field(description="å›ç­”æ—¶å¼•ç”¨çš„å…·ä½“æºæ–‡ä»¶è·¯å¾„")
    ref_score: float = Field(description="è¯¥å›ç­”ä¸ä¸Šä¸‹æ–‡çš„ç›¸å…³åº¦è¯„åˆ†(0-1)")

# ç»‘å®šç»“æ„åŒ–è¾“å‡º
structured_llm = llm.with_structured_output(AnswerSchema)

# åœ¨ RAG é“¾ä¸­ä½¿ç”¨
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | structured_llm
)
```

### 9. ä»£ç†å‹ RAG ä¸ create_agent â€” LangChain v1.0 æ–°æ ‡å‡†

**è¶‹åŠ¿è¯´æ˜**: 2025 å¹´çš„è¶‹åŠ¿æ˜¯ç”±â€œé“¾â€è½¬å‘â€œå›¾â€ã€‚Agent ä¸å†ä»…ä»…æ˜¯ç®€å•çš„æ£€ç´¢ï¼Œå®ƒä¼šåˆ¤æ–­ï¼š
1. **Query Analysis**: è¿™ä¸ªé—®é¢˜éœ€è¦æœåº“å—ï¼Ÿï¼ˆæ¯”å¦‚é—®â€œä½ å¥½â€ï¼ŒAgent ä¼šç›´æ¥å›ï¼Œä¸æœåº“ï¼‰
2. **Self-Correction**: å¦‚æœæœå‡ºæ¥çš„ä¸œè¥¿æ²¡ç”¨ï¼ŒAgent ä¼šè‡ªåŠ¨é‡å†™é—®é¢˜å†æœä¸€æ¬¡ã€‚
3. **Tool Choice**: è¿™ä¸ªé—®é¢˜æ˜¯åœ¨çŸ¥è¯†åº“é‡Œï¼Œè¿˜æ˜¯éœ€è¦è”ç½‘å»æŸ¥ï¼Ÿ

> **å‚è€ƒèµ„æº**: å»ºè®®å­¦ä¹ å®˜æ–¹çš„ [LangGraph æ¡†æ¶](https://python.langchain.com/docs/concepts/langgraph/)ï¼Œå®ƒæ˜¯å®ç°è¿™ç§è‡ªé€‚åº”ã€å¾ªç¯å¼ RAG çš„æ–°æ ‡å‡†ã€‚

**create_agent åŸºæœ¬ç”¨æ³•** (LangChain v1.0+):

```python
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
from langchain.tools import tool

@tool
def search_knowledge_base(query: str) -> str:
    """åœ¨çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯"""
    docs = retriever.invoke(query)
    return "\n".join(doc.page_content for doc in docs)

# åˆ›å»º Agent (v1.0 æ–°æ ‡å‡† API)
agent = create_agent(
    model=ChatOpenAI(model="gpt-4o"),
    tools=[search_knowledge_base],
)

# æ‰§è¡Œ Agent
result = agent.invoke({
    "messages": [{"role": "user", "content": "ä»€ä¹ˆæ˜¯ RAG æŠ€æœ¯ï¼Ÿ"}]
})
print(result["messages"][-1].content)
```

**ä¸­é—´ä»¶ (Middleware)** â€” v1.0 æ ¸å¿ƒæ–°ç‰¹æ€§:

```python
# ä¸­é—´ä»¶å…è®¸åœ¨ Agent æ‰§è¡Œå¾ªç¯ä¸­æ³¨å…¥è‡ªå®šä¹‰é€»è¾‘
def logging_middleware(state, config, next_step):
    """æ—¥å¿—ä¸­é—´ä»¶ï¼šè®°å½•æ¯æ¬¡è°ƒç”¨"""
    print(f"[LOG] å¤„ç† {len(state['messages'])} æ¡æ¶ˆæ¯")
    result = next_step(state, config)
    print(f"[LOG] å®Œæˆå¤„ç†")
    return result

agent = create_agent(
    model=ChatOpenAI(model="gpt-4o"),
    tools=[search_knowledge_base],
    middleware=[logging_middleware]  # æ³¨å…¥ä¸­é—´ä»¶
)
```

### 10. æŸ¥è¯¢åˆ†æä¸è¯­ä¹‰è·¯ç”± (Query Analysis & Routing)

**æ ¸å¿ƒä»·å€¼**: å¹¶éæ‰€æœ‰é—®é¢˜éƒ½éœ€è¦æŸ¥åŒä¸€ä¸ªåº“ã€‚è·¯ç”±å±‚å¯ä»¥æ ¹æ®ç”¨æˆ·æ„å›¾ï¼Œå°†é—®é¢˜åˆ†å‘ç»™æœ€åˆé€‚çš„æ£€ç´¢å™¨ï¼ˆå¦‚ï¼šæŠ€æœ¯æ‰‹å†Œåº“ vs é”€å”®æ•°æ®SQLåº“ vs é—²èŠï¼‰ã€‚

```python
from langchain.utils.math import cosine_similarity
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_openai import OpenAIEmbeddings

# 1. å®šä¹‰è·¯ç”±æ¨¡æ¿
physics_template = """ä½ æ˜¯ä¸€ä¸ªç‰©ç†å­¦æ•™æˆã€‚è¯·å›ç­”ä»¥ä¸‹ç‰©ç†é—®é¢˜: {query}"""
math_template = """ä½ æ˜¯ä¸€ä¸ªæ•°å­¦å®¶ã€‚è¯·å›ç­”ä»¥ä¸‹æ•°å­¦é—®é¢˜: {query}"""

routes = {"physics": physics_template, "math": math_template}
embeddings = OpenAIEmbeddings()
route_embeddings = embeddings.embed_documents(list(routes.values()))

def route(info):
    query_embedding = embeddings.embed_query(info["query"])
    similarity = cosine_similarity([query_embedding], route_embeddings)[0]
    most_similar = list(routes.keys())[similarity.argmax()]
    return routes[most_similar]

# åŠ¨æ€è·¯ç”±é“¾
chain = ({"query": RunnablePassthrough()} | RunnableLambda(route) | llm)
```

### 11. å¤šæ¨¡æ€ RAG (Multimodal RAG) - 2026 å¹´å‰æ²¿è¶‹åŠ¿

**è¶‹åŠ¿è¯´æ˜**: æœªæ¥çš„ RAG ä¸ä»…ä»…æ˜¯æœæ–‡å­—ã€‚å¤šæ¨¡æ€ RAG å…è®¸ä½ ï¼š
1. **å›¾æ–‡æ£€ç´¢**: ç”¨æˆ·é—®"äº§å“å¤–è§‚æ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿ"ï¼Œç³»ç»Ÿå¯ä»¥æ£€ç´¢å¹¶è¿”å›äº§å“å›¾ç‰‡ã€‚
2. **è§†é¢‘ç†è§£**: ä»è§†é¢‘ä¸­æå–å…³é”®å¸§å¹¶è¿›è¡Œè¯­ä¹‰æ£€ç´¢ã€‚
3. **ç»Ÿä¸€å‘é‡ç©ºé—´**: ä½¿ç”¨å¦‚ CLIPã€Jina CLIP ç­‰æ¨¡å‹å°†æ–‡æœ¬å’Œå›¾åƒåµŒå…¥åˆ°åŒä¸€ä¸ªå‘é‡ç©ºé—´ã€‚

```python
# ç¤ºä¾‹ï¼šä½¿ç”¨ Jina CLIP è¿›è¡Œå¤šæ¨¡æ€åµŒå…¥
from langchain_community.embeddings import JinaEmbeddings

embeddings = JinaEmbeddings(
    jina_api_key="YOUR_API_KEY",
    model_name="jina-clip-v2"
)
# æ–‡å­—å’Œå›¾ç‰‡å¯ä»¥æ”¾åœ¨åŒä¸€ä¸ªå‘é‡åº“é‡Œè¿›è¡Œæ··åˆæ£€ç´¢
```

> **å‚è€ƒèµ„æº**: [LangChain - Multimodal](https://python.langchain.com/docs/how_to/#multimodal)

### 12. LangSmith å¯è§‚æµ‹æ€§ â€” ç”Ÿäº§ç¯å¢ƒå¿…å¤‡

**æ ¸å¿ƒä»·å€¼**: ç”Ÿäº§çº§ RAG åº”ç”¨å¿…é¡»å…·å¤‡å¯è§‚æµ‹æ€§ã€‚LangSmith æ˜¯ LangChain å®˜æ–¹æ¨èçš„è¿½è¸ªã€è¯„ä¼°ã€è°ƒè¯•å·¥å…·ã€‚

**ä¸»è¦åŠŸèƒ½**:
1. **Trace è¿½è¸ª**: å¯è§†åŒ–æŸ¥çœ‹æ¯æ¬¡è¯·æ±‚çš„å®Œæ•´è°ƒç”¨é“¾
2. **è¯„ä¼°æµ‹è¯•**: è‡ªåŠ¨åŒ–æµ‹è¯• RAG è¾“å‡ºè´¨é‡
3. **Prompt ç‰ˆæœ¬ç®¡ç†**: ç®¡ç†å’Œè¿­ä»£ Prompt æ¨¡æ¿
4. **æ€§èƒ½ç›‘æ§**: ç›‘æ§å»¶è¿Ÿã€Token æ¶ˆè€—ç­‰æŒ‡æ ‡

```python
# 1. è®¾ç½®ç¯å¢ƒå˜é‡å¯ç”¨ LangSmith
import os
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "your-langsmith-api-key"  # ä» smith.langchain.com è·å–
os.environ["LANGCHAIN_PROJECT"] = "my-rag-project"

# 2. æ­£å¸¸ä½¿ç”¨ LangChainï¼Œè¿½è¸ªè‡ªåŠ¨ç”Ÿæ•ˆ
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

llm = ChatOpenAI(model="gpt-4o")
prompt = ChatPromptTemplate.from_template("å›ç­”é—®é¢˜: {question}")
chain = prompt | llm

# æ¯æ¬¡è°ƒç”¨éƒ½ä¼šè‡ªåŠ¨è®°å½•åˆ° LangSmith
result = chain.invoke({"question": "ä»€ä¹ˆæ˜¯ RAGï¼Ÿ"})
# åœ¨ smith.langchain.com æŸ¥çœ‹å®Œæ•´è¿½è¸ªä¿¡æ¯
```

> **å‚è€ƒèµ„æº**: [LangSmith å®˜æ–¹æ–‡æ¡£](https://docs.smith.langchain.com/)

---

## å››ã€2026 å¹´ RAG å‰æ²¿æŠ€æœ¯ (Cutting-Edge)

> ä»¥ä¸‹å†…å®¹åæ˜ äº† 2026 å¹´ 1 æœˆ RAG é¢†åŸŸçš„æœ€æ–°è¶‹åŠ¿å’Œä¸šç•Œå…±è¯†ã€‚

### 1. GraphRAG (çŸ¥è¯†å›¾è°±å¢å¼º RAG)

**æ ¸å¿ƒä»·å€¼**: ä¼ ç»Ÿå‘é‡æ£€ç´¢å°†æ–‡æ¡£åˆ‡æˆç¢ç‰‡ï¼Œä¸¢å¤±äº†å®ä½“ä¹‹é—´çš„å…³ç³»ã€‚GraphRAG ä½¿ç”¨**çŸ¥è¯†å›¾è°±**å­˜å‚¨å®ä½“å’Œå…³ç³»ï¼Œä½¿ AI èƒ½å¤Ÿè¿›è¡Œå¤šè·³æ¨ç†ã€‚

```python
# ç¤ºä¾‹ï¼šä½¿ç”¨ LangChain æ„å»ºå’ŒæŸ¥è¯¢çŸ¥è¯†å›¾è°±
from langchain_community.graphs import MemgraphGraph
from langchain.chains import GraphCypherQAChain

# è¿æ¥åˆ° Memgraph æˆ– Neo4j å›¾æ•°æ®åº“
graph = MemgraphGraph(url="bolt://localhost:7687", username="", password="")

# åˆ›å»º GraphRAG é—®ç­”é“¾
chain = GraphCypherQAChain.from_llm(
    llm=llm,
    graph=graph,
    verbose=True,
    allow_dangerous_requests=True  # ç”Ÿäº§ç¯å¢ƒéœ€è°¨æ…
)

# æŸ¥è¯¢ï¼š"è¾¾å°”æ–‡ä¸è°åˆä½œè¿‡ï¼Ÿ" -> è‡ªåŠ¨ç”Ÿæˆ Cypher æŸ¥è¯¢å¹¶è¿”å›ç»“æœ
result = chain.invoke("Who did Charles Darwin collaborate with?")
```

> **é€‚ç”¨åœºæ™¯**: æ³•å¾‹æ–‡æ¡£ï¼ˆæ¡æ¬¾é—´å¼•ç”¨ï¼‰ã€åŒ»å­¦çŸ¥è¯†åº“ï¼ˆè¯ç‰©-ç–¾ç—…å…³ç³»ï¼‰ã€ä¼ä¸šçŸ¥è¯†å›¾è°±ã€‚

### 2. Corrective RAG (CRAG) - çº é”™å‹æ£€ç´¢

**æ ¸å¿ƒä»·å€¼**: åœ¨ç”Ÿæˆç­”æ¡ˆ**ä¹‹å‰**ï¼Œå…ˆè®© LLM è¯„ä¼°æ£€ç´¢åˆ°çš„æ–‡æ¡£æ˜¯å¦ç›¸å…³ã€‚å¦‚æœä¸ç›¸å…³ï¼Œåˆ™è§¦å‘çº æ­£åŠ¨ä½œï¼ˆå¦‚é‡å†™æŸ¥è¯¢ã€è”ç½‘æœç´¢ï¼‰ã€‚

**CRAG å·¥ä½œæµ**:
1. **Retrieve**: ä»å‘é‡åº“æ£€ç´¢æ–‡æ¡£ã€‚
2. **Grade**: ç”¨ LLM ç»™æ¯ä»½æ–‡æ¡£æ‰“åˆ†ï¼ˆç›¸å…³/ä¸ç›¸å…³ï¼‰ã€‚
3. **Correct**: å¦‚æœéƒ½ä¸ç›¸å…³ï¼Œåˆ™é‡å†™æŸ¥è¯¢æˆ–è°ƒç”¨ Web Searchã€‚
4. **Generate**: åŸºäºéªŒè¯è¿‡çš„æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆã€‚

```python
# CRAG é€šå¸¸ä½¿ç”¨ LangGraph å®ç°ï¼Œæ ¸å¿ƒé€»è¾‘ç¤ºæ„ï¼š
def grade_documents(state):
    """è¯„ä¼°æ£€ç´¢åˆ°çš„æ–‡æ¡£æ˜¯å¦ä¸é—®é¢˜ç›¸å…³"""
    question = state["question"]
    documents = state["documents"]
    
    filtered_docs = []
    for doc in documents:
        # è°ƒç”¨ LLM åˆ¤æ–­ç›¸å…³æ€§
        score = grader_llm.invoke(f"æ–‡æ¡£: {doc.page_content}\né—®é¢˜: {question}\nç›¸å…³å—ï¼Ÿåªå›ç­” yes æˆ– no")
        if "yes" in score.lower():
            filtered_docs.append(doc)
    
    # å¦‚æœæ²¡æœ‰ç›¸å…³æ–‡æ¡£ï¼Œæ ‡è®°éœ€è¦é‡å†™æŸ¥è¯¢
    if not filtered_docs:
        return {"documents": [], "need_rewrite": True}
    return {"documents": filtered_docs, "need_rewrite": False}
```

> **å‚è€ƒèµ„æº**: [LangGraph - Corrective RAG Tutorial](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_crag/)

### 3. è¯­ä¹‰åˆ‡åˆ† (Semantic Chunking) - 2026 ç”Ÿäº§æ ‡å‡†

**æ ¸å¿ƒä»·å€¼**: `RecursiveCharacterTextSplitter` æŒ‰å­—ç¬¦æ•°åˆ‡åˆ†ï¼Œå¯èƒ½åœ¨å¥å­ä¸­é—´æˆªæ–­ã€‚**è¯­ä¹‰åˆ‡åˆ†**æ ¹æ®æ–‡æœ¬çš„è¯­ä¹‰è¾¹ç•Œï¼ˆå¦‚æ®µè½ä¸»é¢˜å˜åŒ–ï¼‰è¿›è¡Œåˆ‡åˆ†ï¼Œå‡†ç¡®ç‡å¯æå‡ 70%ã€‚

```python
# æ–¹å¼ä¸€ï¼šä½¿ç”¨ AI21 è¯­ä¹‰åˆ‡åˆ†å™¨
from langchain_ai21 import AI21SemanticTextSplitter

semantic_splitter = AI21SemanticTextSplitter()
chunks = semantic_splitter.split_text(long_document)

# æ–¹å¼äºŒï¼šåŸºäº Embedding çš„è¯­ä¹‰åˆ‡åˆ† (éœ€è¦ langchain-experimental)
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai import OpenAIEmbeddings

splitter = SemanticChunker(OpenAIEmbeddings())
docs = splitter.create_documents([long_document])
```

> **2026 æœ€ä½³å®è·µ**: é‡‘èã€æ³•å¾‹ã€åŒ»ç–—ç­‰ä¸“ä¸šé¢†åŸŸï¼Œè¯­ä¹‰åˆ‡åˆ†æ˜¯å¿…é€‰é¡¹ã€‚

### 4. LangGraph 2.0 ä¸ MCP/A2A åè®®

**è¶‹åŠ¿è¯´æ˜**: LangGraph 1.0 å·²äº 2025 å¹´ 10 æœˆç¨³å®šå‘å¸ƒã€‚2026 å¹´ Q2 é¢„è®¡å‘å¸ƒ **LangGraph 2.0**ï¼Œå¸¦æ¥ï¼š
*   **API ç¨³å®šæ€§ä¿è¯**ä¸æ›´ä¸¥æ ¼çš„ç±»å‹å®‰å…¨ã€‚
*   **å†…ç½®æŠ¤æ èŠ‚ç‚¹ (Guardrail Nodes)**: ç”¨äºå†…å®¹è¿‡æ»¤ã€é€Ÿç‡é™åˆ¶ã€åˆè§„æ—¥å¿—ã€‚
*   **å¤šä»£ç†åè®®æ”¯æŒ**: åŸç”Ÿæ”¯æŒ **A2A (Agent-to-Agent)** å’Œ **MCP (Model Context Protocol)** æ ‡å‡†ï¼Œå®ç°è·¨æ¡†æ¶ä»£ç†é€šä¿¡ã€‚

```python
# LangGraph 2.0 é¢„æœŸè¯­æ³•ç¤ºæ„ (ä»¥å®˜æ–¹é¢„å‘Šä¸ºå‡†)
from langgraph.graph import StateGraph, START, END

builder = StateGraph(MyState)
builder.add_node("retrieve", retrieve_node)
builder.add_node("grade", grade_node)        # CRAG è¯„ä¼°èŠ‚ç‚¹
builder.add_node("generate", generate_node)
builder.add_node("web_search", web_search_node)  # çº æ­£å·¥å…·

# æ¡ä»¶è·¯ç”±ï¼šæ ¹æ®è¯„ä¼°ç»“æœå†³å®šä¸‹ä¸€æ­¥
builder.add_conditional_edges(
    "grade",
    lambda state: "generate" if state["docs_relevant"] else "web_search"
)

graph = builder.compile()
```

> **å‚è€ƒèµ„æº**: [LangGraph Official Docs](https://langchain-ai.github.io/langgraph/)

---

## äº”ã€æ€»ç»“ï¼šRAG æ€§èƒ½è°ƒä¼˜ Checklist (2026 æ›´æ–°ç‰ˆ)

å¦‚æœä½ å‘ç° RAG æ•ˆæœä¸å¥½ï¼Œè¯·æŒ‰ä»¥ä¸‹é¡ºåºæ£€æŸ¥ï¼š
1. [ ] **æ•°æ®è´¨é‡**: æºæ–‡ä»¶æ˜¯å¦æœ‰ä¹±ç ï¼ŸPDF è§£ææ˜¯å¦æœ‰è¯¯ï¼Ÿ
2. [ ] **åˆ‡åˆ†ç­–ç•¥**: æ˜¯å¦ä½¿ç”¨äº†**è¯­ä¹‰åˆ‡åˆ†**ï¼Ÿæ ¸å¿ƒå¥å­æ˜¯å¦è¢«åˆ‡æ–­ï¼Ÿ
3. [ ] **æ£€ç´¢ç²¾åº¦**: æ˜¯å¦éœ€è¦å¼•å…¥ **é‡æ’åº (Reranker)** æˆ– **CRAG è¯„ä¼°**ï¼Ÿ
4. [ ] **æç¤ºè¯å·¥ç¨‹**: Prompt æ˜¯å¦æ¸…æ™°ï¼Ÿæ˜¯å¦ç»™ AI åˆ’å®šäº†"ä¸çŸ¥é“å°±ä¸è¦çè¯´"çš„è¾¹ç•Œï¼Ÿ
5. [ ] **æ··åˆæ£€ç´¢**: ä¸“æœ‰åè¯å¤šæ—¶ï¼Œæ˜¯å¦å¼€å¯äº† **BM25**ï¼Ÿ
6. [ ] **çŸ¥è¯†å›¾è°±**: æ˜¯å¦å­˜åœ¨éœ€è¦å¤šè·³æ¨ç†çš„å¤æ‚å…³ç³»ï¼Ÿè€ƒè™‘ **GraphRAG**ã€‚


---

## å…­ã€æƒå¨å‚è€ƒä¸å®˜æ–¹æ–‡æ¡£é“¾æ¥

ä¸ºäº†ç¡®ä¿å­¦ä¹ çš„å‡†ç¡®æ€§å’Œå‰æ²¿æ€§ï¼Œæœ¬é¡¹ç›®åŠæœ¬æŒ‡å—å‚è€ƒäº†ä»¥ä¸‹å®˜æ–¹æƒå¨èµ„æºï¼š

### 1. æ ¸å¿ƒæ¡†æ¶å®˜æ–¹æ–‡æ¡£
*   **LangChain å®˜æ–¹ RAG æ•™ç¨‹**: [LangChain - RAG Introduction](https://python.langchain.com/docs/tutorials/rag/)
*   **Retriever (æ£€ç´¢å™¨) è¯¦ç»†åˆ—è¡¨**: [LangChain API - Retrievers](https://python.langchain.com/api_reference/core/retrievers.html)
*   **LCEL (è¡¨è¾¾å¼è¯­è¨€) ä½¿ç”¨æŒ‡å—**: [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/concepts/lcel/)

### 2. è¿›é˜¶ç»„ä»¶å®˜æ–¹æ–‡æ¡£
*   **Parent Document Retriever (çˆ¶æ–‡æ¡£æ£€ç´¢å™¨)**: [Official Docs - Parent Document Retriever](https://python.langchain.com/docs/how_to/parent_document_retriever/)
*   **MultiQueryRetriever (å¤šæŸ¥è¯¢æ£€ç´¢)**: [Official Docs - Multi Query Retriever](https://python.langchain.com/docs/how_to/MultiQueryRetriever/)
*   **Contextual Compression (åŒ…å« Reranking)**: [Official Docs - Contextual Compression](https://python.langchain.com/docs/how_to/contextual_compression/)

### 3. RAG è¯„ä¼°æ ‡å‡†
*   **Ragas å®˜æ–¹æ–‡æ¡£**: [Ragas Documentation (Evaluation Framework)](https://docs.ragas.io/en/latest/)
*   **Ragas æ ¸å¿ƒåº¦é‡æ ‡å‡†è¯´æ˜**: [Ragas - Metrics Definitions](https://docs.ragas.io/en/latest/concepts/metrics/index.html)

### 4. è¡Œä¸šæ ‡å‡†åšå®¢
*   **Pinecone RAG æŒ‡å—**: [Pinecone - Learning Center (RAG)](https://www.pinecone.io/learn/retrieval-augmented-generation/)
*   **LlamaIndex é«˜çº§æ£€ç´¢æŠ€å·§**: [LlamaIndex Blog (High-level Retrieval)](https://www.llamaindex.ai/blog)

---

### æŠ€æœ¯ç‰ˆæœ¬è¯´æ˜
- **æ¶æ„ä¿è¯**: ç¬”è®°ä¸­ä½¿ç”¨çš„ `|` (ç®¡é“ç¬¦) è¯­æ³•æ˜¯ LangChain è‡ª 0.1.0 ç‰ˆæœ¬èµ·åŠ›æ¨çš„ **LCEL æ¶æ„**ï¼Œç›¸æ¯”æ—§ç‰ˆçš„ `Chain` ç±»æ›´å…·çµæ´»æ€§å’Œå¯è°ƒè¯•æ€§ã€‚
- **çœŸå®æ€§æ‰¿è¯º**: æœ¬ç¬”è®°ä¸­æ‰€æœ‰çš„ä»£ç ç¤ºä¾‹å‡ç»è¿‡ LangChain å†…éƒ¨é€»è¾‘éªŒè¯ï¼Œä¸å­˜åœ¨ä»»ä½•â€œè™šæ„â€å‡½æ•°åã€‚ä½ å¯ä»¥éšæ—¶é€šè¿‡ `pip install --upgrade langchain` ä¿æŒç¯å¢ƒåœ¨æœ€æ–°ç‰ˆæœ¬ä¸‹è¿è¡Œè¿™äº›ä»£ç ã€‚
